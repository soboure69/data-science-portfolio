{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# J3 — Évaluation + courbes d'entraînement + sauvegarde\n",
        "\n",
        "Objectifs :\n",
        "- ré-entraîner rapidement (pour récupérer un `history`)\n",
        "- évaluer sur **test**\n",
        "- sauvegarder des **courbes** (loss/accuracy/roc_auc) dans `results/`\n",
        "- sauvegarder les **métriques** dans `results/metrics.json`\n",
        "\n",
        "Note : le modèle final est sauvegardé dans `models/`. (Les dossiers `models/` et `*.keras` sont ignorés par git.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cwd = Path.cwd().resolve()\n",
        "PROJECT_DIR = None\n",
        "for p in [cwd] + list(cwd.parents):\n",
        "    if (p / 'src').exists():\n",
        "        PROJECT_DIR = p\n",
        "        break\n",
        "if PROJECT_DIR is None:\n",
        "    raise RuntimeError(f\"Could not find project root containing 'src' starting from: {cwd}\")\n",
        "sys.path.insert(0, str(PROJECT_DIR))\n",
        "\n",
        "from src.text_preprocessing import TextPreprocessor\n",
        "from src.model_architecture import ModelConfig, build_bilstm_model\n",
        "\n",
        "MODELS_DIR = PROJECT_DIR / 'models'\n",
        "RESULTS_DIR = PROJECT_DIR / 'results'\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Project dir:', PROJECT_DIR)\n",
        "print('TensorFlow:', tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Charger IMDB (train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_WORDS = 10_000\n",
        "MAX_LEN = 200\n",
        "VAL_SIZE = 5_000\n",
        "\n",
        "pre = TextPreprocessor(max_words=MAX_WORDS, max_len=MAX_LEN)\n",
        "data = pre.load_imdb_text(validation_size=VAL_SIZE, seed=42)\n",
        "\n",
        "X_train, y_train = data.X_train, data.y_train\n",
        "X_val, y_val = data.X_val, data.y_val\n",
        "X_test, y_test = data.X_test, data.y_test\n",
        "\n",
        "print('X_train:', X_train.shape)\n",
        "print('X_val  :', X_val.shape)\n",
        "print('X_test :', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) (Re)training pour obtenir `history` + sauvegarde modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = ModelConfig(\n",
        "    vocab_size=MAX_WORDS,\n",
        "    max_len=MAX_LEN,\n",
        "    embedding_dim=128,\n",
        "    rnn_units=64,\n",
        "    dropout=0.3,\n",
        ")\n",
        "model = build_bilstm_model(cfg)\n",
        "\n",
        "weights_ckpt_path = MODELS_DIR / 'sentiment_model.weights.h5'\n",
        "final_model_path = MODELS_DIR / 'sentiment_model.keras'\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(weights_ckpt_path),\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    ),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=6,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "if weights_ckpt_path.exists():\n",
        "    model.load_weights(str(weights_ckpt_path))\n",
        "model.save(str(final_model_path))\n",
        "\n",
        "print('Saved model:', final_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Évaluer sur test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_metrics = model.evaluate(X_test, y_test, verbose=0)\n",
        "metrics_dict = {name: float(val) for name, val in zip(model.metrics_names, test_metrics)}\n",
        "metrics_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Courbes d'entraînement (loss / accuracy / roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(hist, key, title, out_path: Path):\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.plot(hist.history.get(key, []), label=key)\n",
        "    plt.plot(hist.history.get(f'val_{key}', []), label=f'val_{key}')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(key)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "plot_history(history, 'loss', 'Training vs Validation Loss', RESULTS_DIR / 'training_loss.png')\n",
        "plot_history(history, 'accuracy', 'Training vs Validation Accuracy', RESULTS_DIR / 'training_accuracy.png')\n",
        "plot_history(history, 'roc_auc', 'Training vs Validation ROC-AUC', RESULTS_DIR / 'training_roc_auc.png')\n",
        "\n",
        "print('Saved plots to:', RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Sauvegarder les métriques dans results/metrics.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_json = RESULTS_DIR / 'metrics.json'\n",
        "payload = {\n",
        "    'model': 'BiLSTM',\n",
        "    'test_metrics': metrics_dict,\n",
        "    'config': {\n",
        "        'max_words': MAX_WORDS,\n",
        "        'max_len': MAX_LEN,\n",
        "        'embedding_dim': cfg.embedding_dim,\n",
        "        'rnn_units': cfg.rnn_units,\n",
        "        'dropout': cfg.dropout,\n",
        "    },\n",
        "}\n",
        "out_json.write_text(json.dumps(payload, indent=2), encoding='utf-8')\n",
        "print('Saved:', out_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Fin de J3 :\n",
        "- `results/metrics.json`\n",
        "- `results/training_loss.png`, `results/training_accuracy.png`, `results/training_roc_auc.png`\n",
        "- modèle sauvegardé dans `models/`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
