# ğŸš€ Data Science Portfolio - Guide Complet de RÃ©alisation des 4 Projets

> **Guide ultime** : De la conception Ã  la production pour 4 projets data qui impressionnent les recruteurs. Temps total : 40-54h. Resultat : Portfolio visible et valorisÃ©.

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture Portfolio](#architecture-portfolio)
3. [Projet #1 : ML Classique - PrÃ©diction Churn](#projet-1--ml-classique---prÃ©diction-churn)
4. [Projet #2 : Deep Learning - NLP Sentiment](#projet-2--deep-learning---npl-sentiment)
5. [Projet #3 : Data Engineering - ETL Pipeline](#projet-3--data-engineering---etl-pipeline)
6. [Projet #4 : Business-Focused - Dashboard Recommandation](#projet-4--business-focused---dashboard-recommandation)
7. [DÃ©ploiement & Production](#dÃ©ploiement--production)
8. [Site Web Portfolio Interactif](#site-web-portfolio-interactif)
9. [Checklist Finalisation](#checklist-finalisation)

---

## ğŸ¯ Vue d'Ensemble

### Objectifs StratÃ©giques

Ce portfolio dÃ©montre 4 compÃ©tences cruciales que les recruteurs Data 2026 cherchent :

| CompÃ©tence | Projet | Preuve | Impact Recruteur |
|-----------|--------|--------|-----------------|
| **Machine Learning** | Proj #1 | ModÃ¨les production-ready | +40% callbacks |
| **Deep Learning** | Proj #2 | ModÃ¨le dÃ©ployÃ© live | DÃ©montre scalabilitÃ© |
| **Data Engineering** | Proj #3 | Pipeline automatisÃ© | CompÃ©tence rare |
| **Business Impact** | Proj #4 | App interactive utilisateurs | Pense mÃ©tier |

### Stack Technique Finale

```
Frontend: HTML/CSS/JavaScript + Streamlit/Dash
Backend: Python 3.10+ (pandas, scikit-learn, TensorFlow)
ML: scikit-learn, XGBoost, TensorFlow, PyTorch
Data: SQL, PostgreSQL, MongoDB
Deployment: GitHub Pages, Heroku, Streamlit Cloud
APIs: Twitter API, Reddit API, OpenWeatherMap API
Visualization: Plotly, Matplotlib, Seaborn
```

### Timeline RÃ©aliste

```
Semaine 1-2 : Projet #1 (ML Classique) - 8-12h
Semaine 2-3 : Projet #2 (Deep Learning) - 12-16h
Semaine 4 : Projet #3 (Data Engineering) - 10-14h
Semaine 4-5 : Projet #4 (Dashboard) - 8-12h
Semaine 5-6 : Site Web Portfolio - 8-10h
Total : 46-64h / 6 semaines
```

---

## ğŸ“ Architecture Portfolio

### Structure GitHub Optimale

```
data-science-portfolio/
â”‚
â”œâ”€â”€ README.md (ce fichier)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ 01_ML_Classique_Churn/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”‚   â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â””â”€â”€ 01_churn_prediction.ipynb
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”‚   â”œâ”€â”€ model_training.py
â”‚   â”‚   â””â”€â”€ evaluation.py
â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ model_metrics.json
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â””â”€â”€ feature_importance.png
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ Churn_Prediction_Report.pdf
â”‚
â”œâ”€â”€ 02_DL_NLP_Sentiment/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”‚   â””â”€â”€ imdb_reviews.csv
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â””â”€â”€ 02_sentiment_analysis.ipynb
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_preprocessing.py
â”‚   â”‚   â”œâ”€â”€ model_architecture.py
â”‚   â”‚   â””â”€â”€ inference.py
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ sentiment_model.h5
â”‚   â”‚   â””â”€â”€ tokenizer.pickle
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ training_history.png
â”‚   â”‚   â””â”€â”€ confusion_matrix.png
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 03_Data_Engineering_Pipeline/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_extractors.py (Twitter, Reddit, Weather APIs)
â”‚   â”‚   â”œâ”€â”€ transformers.py (nettoyage, validation)
â”‚   â”‚   â”œâ”€â”€ loaders.py (PostgreSQL/MongoDB)
â”‚   â”‚   â””â”€â”€ orchestrator.py (Luigi/Airflow DAGs)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ airflow_config.yaml
â”‚   â”‚   â””â”€â”€ api_credentials_template.env
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ data_pipeline_dag.py
â”‚   â”œâ”€â”€ sql_scripts/
â”‚   â”‚   â”œâ”€â”€ schema_creation.sql
â”‚   â”‚   â””â”€â”€ data_quality_checks.sql
â”‚   â”œâ”€â”€ Architecture_Diagram.png
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 04_Dashboard_Recommendation/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ products_data.csv
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ recommendation_engine.py
â”‚   â”‚   â””â”€â”€ data_loader.py
â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â”œâ”€â”€ app.py (Plotly/Dash app)
â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â”œâ”€â”€ style.css
â”‚   â”‚   â”‚   â””â”€â”€ images/
â”‚   â”‚   â””â”€â”€ callbacks/
â”‚   â”‚       â””â”€â”€ interaction_handlers.py
â”‚   â”œâ”€â”€ Business_Case.pdf
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ portfolio_website/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ projects.html
â”‚   â”œâ”€â”€ about.html
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ main.js
â”‚   â””â”€â”€ assets/
â”‚       â”œâ”€â”€ images/
â”‚       â””â”€â”€ screenshots/
â”‚
â””â”€â”€ requirements.txt
```

### Fichier requirements.txt Global

```txt
# Core Data Science
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.3.0
scipy==1.11.2

# Machine Learning
xgboost==2.0.0
lightgbm==4.0.0
catboost==1.2.0

# Deep Learning
tensorflow==2.13.0
torch==2.0.1
torchvision==0.15.1
nltk==3.8.1
transformers==4.31.0

# Data Engineering
sqlalchemy==2.0.20
psycopg2-binary==2.9.7
pymongo==4.5.0
apache-airflow==2.6.3
luigi==3.4.0

# APIs & Data Collection
tweepy==4.14.0
praw==7.7.0
requests==2.31.0
beautifulsoup4==4.12.2

# Visualization
plotly==5.16.1
dash==2.13.0
matplotlib==3.7.2
seaborn==0.12.2

# Web Deployment
streamlit==1.27.0
flask==2.3.2
gunicorn==21.2.0

# Utilities
python-dotenv==1.0.0
jupyter==1.0.0
jupyterlab==4.0.4
ipython==8.14.0
```

---

## ğŸ¤– Projet #1 : ML Classique - PrÃ©diction Churn

### ğŸ“Œ Objectifs

- âœ… PrÃ©dire clients qui vont quitter (churn)
- âœ… Comparer Random Forest + XGBoost
- âœ… Atteindre 85%+ accuracy
- âœ… Livrable : Notebook Jupyter + PDF report + mÃ©triques

### â±ï¸ DurÃ©e : 8-12 heures

### ğŸ”„ Ã‰tapes DÃ©taillÃ©es

#### Ã‰tape 1 : Setup & Data Loading (1-2h)

```python
# 01_ML_Classique_Churn/src/data_processing.py

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import warnings
warnings.filterwarnings('ignore')

class ChurnDataProcessor:
    """Classe pour charger et traiter les donnÃ©es Telco Customer Churn"""
    
    def __init__(self, filepath):
        self.df = pd.read_csv(filepath)
        self.le_dict = {}
    
    def load_and_explore(self):
        """Charger et afficher infos dataset"""
        print(f"Dataset shape: {self.df.shape}")
        print(f"\nData types:\n{self.df.dtypes}")
        print(f"\nMissing values:\n{self.df.isnull().sum()}")
        print(f"\nChurn distribution:\n{self.df['Churn'].value_counts(normalize=True)}")
        return self.df
    
    def clean_data(self):
        """Nettoyage donnÃ©es"""
        # Supprimer colonnes inutiles
        self.df = self.df.drop(['customerID'], axis=1)
        
        # Convertir Churn en binaire
        self.df['Churn'] = (self.df['Churn'] == 'Yes').astype(int)
        
        # Nettoyer colonnes numÃ©riques
        self.df['TotalCharges'] = pd.to_numeric(
            self.df['TotalCharges'], 
            errors='coerce'
        ).fillna(self.df['TotalCharges'].median())
        
        return self.df
    
    def feature_engineering(self):
        """CrÃ©er nouvelles features"""
        # Tenure groupÃ©s
        self.df['tenure_group'] = pd.cut(
            self.df['tenure'], 
            bins=[0, 12, 24, 48, 72],
            labels=['0-1 ans', '1-2 ans', '2-4 ans', '4+ ans']
        )
        
        # Ratio charges
        self.df['monthly_to_total_ratio'] = (
            self.df['MonthlyCharges'] / (self.df['TotalCharges'] + 1)
        )
        
        # Service counts
        services = ['PhoneService', 'InternetService', 'OnlineSecurity']
        self.df['num_services'] = self.df[services].notna().sum(axis=1)
        
        return self.df
    
    def encode_categorical(self):
        """Encoder variables catÃ©goriques"""
        categorical_cols = self.df.select_dtypes(include=['object']).columns
        
        for col in categorical_cols:
            le = LabelEncoder()
            self.df[col] = le.fit_transform(self.df[col])
            self.le_dict[col] = le
        
        return self.df
    
    def prepare_for_modeling(self):
        """Split et scale donnÃ©es"""
        X = self.df.drop('Churn', axis=1)
        y = self.df['Churn']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=0.2,
            random_state=42,
            stratify=y
        )
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        return X_train_scaled, X_test_scaled, y_train, y_test, scaler

# Usage
processor = ChurnDataProcessor('data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv')
df = processor.load_and_explore()
df = processor.clean_data()
df = processor.feature_engineering()
df = processor.encode_categorical()
X_train, X_test, y_train, y_test, scaler = processor.prepare_for_modeling()
```

#### Ã‰tape 2 : Model Training (3-4h)

```python
# 01_ML_Classique_Churn/src/model_training.py

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
import xgboost as xgb
import json

class ChurnModelTrainer:
    """EntraÃ®ner et comparer modÃ¨les ML"""
    
    def __init__(self, X_train, X_test, y_train, y_test):
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test
        self.models = {}
        self.best_model = None
    
    def train_random_forest(self):
        """Random Forest avec hyperparameter tuning"""
        print("ğŸŒ² Training Random Forest...")
        
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 15, 20],
            'min_samples_split': [5, 10],
            'random_state': [42]
        }
        
        rf = RandomForestClassifier()
        grid_search = GridSearchCV(
            rf, param_grid,
            cv=5,
            scoring='roc_auc',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(self.X_train, self.y_train)
        
        print(f"âœ… Best RF params: {grid_search.best_params_}")
        print(f"âœ… Best RF CV score: {grid_search.best_score_:.4f}")
        
        self.models['RandomForest'] = grid_search.best_estimator_
        return grid_search.best_estimator_
    
    def train_xgboost(self):
        """XGBoost avec tuning"""
        print("âš¡ Training XGBoost...")
        
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [5, 7, 9],
            'learning_rate': [0.01, 0.05],
            'subsample': [0.8, 1.0]
        }
        
        xgb_model = xgb.XGBClassifier(random_state=42)
        grid_search = GridSearchCV(
            xgb_model, param_grid,
            cv=5,
            scoring='roc_auc',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(self.X_train, self.y_train)
        
        print(f"âœ… Best XGB params: {grid_search.best_params_}")
        print(f"âœ… Best XGB CV score: {grid_search.best_score_:.4f}")
        
        self.models['XGBoost'] = grid_search.best_estimator_
        return grid_search.best_estimator_
    
    def train_both(self):
        """EntraÃ®ner RF et XGB, retourner meilleur"""
        self.train_random_forest()
        self.train_xgboost()
        
        return self.models

# Usage
trainer = ChurnModelTrainer(X_train, X_test, y_train, y_test)
models = trainer.train_both()
```

#### Ã‰tape 3 : Evaluation & Metrics (2-3h)

```python
# 01_ML_Classique_Churn/src/evaluation.py

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
import json

class ChurnModelEvaluator:
    """Ã‰valuer et comparer modÃ¨les"""
    
    def __init__(self, models, X_test, y_test):
        self.models = models
        self.X_test = X_test
        self.y_test = y_test
        self.results = {}
    
    def evaluate_all_models(self):
        """Ã‰valuer tous les modÃ¨les"""
        for name, model in self.models.items():
            y_pred = model.predict(self.X_test)
            y_pred_proba = model.predict_proba(self.X_test)[:, 1]
            
            self.results[name] = {
                'accuracy': accuracy_score(self.y_test, y_pred),
                'precision': precision_score(self.y_test, y_pred),
                'recall': recall_score(self.y_test, y_pred),
                'f1': f1_score(self.y_test, y_pred),
                'roc_auc': roc_auc_score(self.y_test, y_pred_proba),
                'confusion_matrix': confusion_matrix(self.y_test, y_pred).tolist()
            }
            
            print(f"\nğŸ“Š {name} Results:")
            print(f"   Accuracy:  {self.results[name]['accuracy']:.4f}")
            print(f"   Precision: {self.results[name]['precision']:.4f}")
            print(f"   Recall:    {self.results[name]['recall']:.4f}")
            print(f"   F1-Score:  {self.results[name]['f1']:.4f}")
            print(f"   ROC-AUC:   {self.results[name]['roc_auc']:.4f}")
        
        return self.results
    
    def visualize_confusion_matrices(self):
        """Visualiser matrices confusion"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        for idx, (name, model) in enumerate(self.models.items()):
            y_pred = model.predict(self.X_test)
            cm = confusion_matrix(self.y_test, y_pred)
            
            sns.heatmap(
                cm, annot=True, fmt='d',
                cmap='Blues', ax=axes[idx],
                cbar_kws={'label': 'Count'}
            )
            axes[idx].set_title(f'{name} - Confusion Matrix')
            axes[idx].set_ylabel('True Label')
            axes[idx].set_xlabel('Predicted Label')
        
        plt.tight_layout()
        plt.savefig('results/confusion_matrix.png', dpi=300, bbox_inches='tight')
        print("âœ… Confusion matrix visualization saved!")
    
    def plot_feature_importance(self):
        """Feature importance"""
        best_model = self.models['XGBoost']  # GÃ©nÃ©ralement meilleur
        
        importance_df = pd.DataFrame({
            'feature': [f'Feature_{i}' for i in range(self.X_test.shape[1])],
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False).head(15)
        
        plt.figure(figsize=(10, 6))
        sns.barplot(data=importance_df, x='importance', y='feature', palette='viridis')
        plt.title('Top 15 Feature Importance (XGBoost)')
        plt.xlabel('Importance Score')
        plt.tight_layout()
        plt.savefig('results/feature_importance.png', dpi=300, bbox_inches='tight')
        print("âœ… Feature importance plot saved!")
    
    def save_metrics_json(self):
        """Sauvegarder mÃ©triques JSON"""
        with open('results/model_metrics.json', 'w') as f:
            json.dump(self.results, f, indent=4)
        print("âœ… Metrics saved to model_metrics.json")

# Usage
evaluator = ChurnModelEvaluator(models, X_test, y_test)
results = evaluator.evaluate_all_models()
evaluator.visualize_confusion_matrices()
evaluator.plot_feature_importance()
evaluator.save_metrics_json()
```

#### Ã‰tape 4 : Jupyter Notebook (2h)

CrÃ©er `notebooks/01_churn_prediction.ipynb` avec structure :

```
1. Imports & Setup
2. Load Data
3. Exploratory Data Analysis (EDA)
   - Distribution churn
   - CorrÃ©lations
   - Outliers
4. Data Preprocessing
   - Missing values
   - Encoding
   - Feature engineering
5. Model Training
   - Random Forest
   - XGBoost
6. Model Evaluation
   - Metrics
   - Visualizations
7. Key Insights
8. Recommendations
```

#### Ã‰tape 5 : PDF Report (1-2h)

CrÃ©er `Churn_Prediction_Report.pdf` avec :

```markdown
# Churn Prediction Model - Executive Report

## Executive Summary
- Objectif: PrÃ©dire clients risque churn
- Meilleur modÃ¨le: XGBoost (87.2% accuracy)
- Business impact: Identifier 85% des clients Ã  risque

## Data Overview
- Dataset: 7,043 customers, 21 features
- Churn rate: 26.5%
- Features: Demographics, services, charges

## Methodology
1. Data cleaning: Missing values, encoding
2. Feature engineering: Tenure groups, service counts
3. Model comparison: RF vs XGBoost
4. Hyperparameter tuning: GridSearchCV

## Results
[Include confusion matrix, feature importance plots]

## Recommendations
- Retargeter top 5% high-risk customers
- Optimize contract terms based on feature importance
- Implement early warning system

## Deployment
- Model ready for production
- Inference latency: <100ms per prediction
```

#### Ã‰tape 6 : README Projet (30 min)

CrÃ©er `01_ML_Classique_Churn/README.md` :

```markdown
# ğŸ¯ Churn Prediction - Machine Learning

## Problem Statement
Predict which customers will churn (leave) using ML models.

## Dataset
- **Source**: Kaggle Telco Customer Churn
- **Size**: 7,043 rows, 21 features
- **Target**: Churn (binary: Yes/No)
- **Churn Rate**: 26.5%

## Features
- Demographics: age, gender, senior citizen
- Services: internet, phone, TV, security
- Financial: monthly charges, total charges, tenure

## Models Compared
1. **Random Forest** - Ensemble trees
   - Accuracy: 86.1%
   - ROC-AUC: 0.825
   
2. **XGBoost** - Gradient boosting
   - Accuracy: 87.2%
   - ROC-AUC: 0.839 âœ… **WINNER**

## Key Insights
1. Month-to-month contracts have 3x higher churn
2. Customers >24 months tenure churn 80% less
3. Fiber optic internet has highest churn rate
4. Annual contracts are most stable

## How to Run

### 1. Install dependencies
```bash
pip install -r ../../requirements.txt
```

### 2. Run notebook
```bash
jupyter notebook notebooks/01_churn_prediction.ipynb
```

### 3. View results
```
results/
â”œâ”€â”€ model_metrics.json
â”œâ”€â”€ confusion_matrix.png
â””â”€â”€ feature_importance.png
```

## Files Structure
```
01_ML_Classique_Churn/
â”œâ”€â”€ data/raw/                # DonnÃ©es brutes
â”œâ”€â”€ notebooks/               # Jupyter notebook
â”œâ”€â”€ src/                     # Code modulaire
â”‚   â”œâ”€â”€ data_processing.py   # Nettoyage & prep
â”‚   â”œâ”€â”€ model_training.py    # EntraÃ®nement
â”‚   â””â”€â”€ evaluation.py        # Ã‰valuation
â”œâ”€â”€ results/                 # Outputs (images, JSON)
â”œâ”€â”€ README.md               # Ce fichier
â””â”€â”€ Churn_Prediction_Report.pdf
```

## Business Impact
- **Accuracy**: 87.2% â†’ Identify 87% of churners correctly
- **Precision**: 78% â†’ 78% of flagged customers actually churn
- **Business Value**: Save $100k+/year by targeting right customers

## Deployment
âœ… Model production-ready
âœ… Inference latency: <100ms
âœ… Can process 10k predictions/hour

## Next Steps
- A/B test retention campaigns
- Implement automated alerting
- Monitor model drift in production
```

### ğŸ“Š RÃ©sultat Attendu

**Fichiers livrables** :
- âœ… Notebook Jupyter avec EDA + training + evaluation
- âœ… PDF Report 5-10 pages avec insights business
- âœ… Code modulaire propre (data_processing.py, model_training.py, evaluation.py)
- âœ… Visualisations (confusion matrix, feature importance)
- âœ… README dÃ©taillÃ©
- âœ… GitHub repo avec toutes ces ressources

**Metrics cibles** :
- Accuracy: 85%+ âœ…
- Precision: 75%+
- ROC-AUC: 0.82+

---

## ğŸ§  Projet #2 : Deep Learning - NLP Sentiment Analysis

### ğŸ“Œ Objectifs

- âœ… Classifier sentiments IMDB reviews (positif/nÃ©gatif)
- âœ… Utiliser LSTM/GRU avec embeddings
- âœ… DÃ©ployer modÃ¨le Streamlit app
- âœ… Livrable : Code + Streamlit live + visualisations

### â±ï¸ DurÃ©e : 12-16 heures

### ğŸ”„ Ã‰tapes DÃ©taillÃ©es

#### Ã‰tape 1 : Data Preparation (2-3h)

```python
# 02_DL_NLP_Sentiment/src/text_preprocessing.py

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import nltk
from nltk.corpus import stopwords
import re

class TextPreprocessor:
    """PrÃ©parer textes pour Deep Learning"""
    
    def __init__(self, max_words=10000, max_len=200):
        self.max_words = max_words
        self.max_len = max_len
        self.tokenizer = None
        nltk.download('stopwords')
        self.stop_words = set(stopwords.words('english'))
    
    def clean_text(self, text):
        """Nettoyer texte brut"""
        # Minuscules
        text = text.lower()
        # Enlever ponctuation
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
        # Enlever stopwords
        words = [w for w in text.split() if w not in self.stop_words]
        return ' '.join(words)
    
    def load_imdb_data(self):
        """Charger IMDB dataset"""
        print("ğŸ“¥ Loading IMDB dataset...")
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(
            num_words=self.max_words
        )
        return (X_train, y_train), (X_test, y_test)
    
    def get_word_index(self):
        """Obtenir word index"""
        word_index = tf.keras.datasets.imdb.get_word_index()
        reverse_word_index = dict([(v, k) for k, v in word_index.items()])
        return word_index, reverse_word_index
    
    def decode_review(self, encoded_review, reverse_word_index):
        """DÃ©coder review encodÃ©e"""
        return ' '.join([
            reverse_word_index.get(i - 3, '?') 
            for i in encoded_review
        ])
    
    def pad_sequences_data(self, sequences):
        """Pad sequences Ã  mÃªme length"""
        return pad_sequences(
            sequences,
            maxlen=self.max_len,
            padding='post',
            truncating='post'
        )

# Load & preprocess
preprocessor = TextPreprocessor(max_words=10000, max_len=200)
(X_train, y_train), (X_test, y_test) = preprocessor.load_imdb_data()

X_train_pad = preprocessor.pad_sequences_data(X_train)
X_test_pad = preprocessor.pad_sequences_data(X_test)

print(f"âœ… Training data shape: {X_train_pad.shape}")
print(f"âœ… Test data shape: {X_test_pad.shape}")
```

#### Ã‰tape 2 : Model Architecture (3-4h)

```python
# 02_DL_NLP_Sentiment/src/model_architecture.py

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Embedding, LSTM, GRU, Dense, Dropout,
    Bidirectional, SpatialDropout1D
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt

class SentimentLSTMModel:
    """ModÃ¨le LSTM pour sentiment analysis"""
    
    def __init__(self, vocab_size=10000, embedding_dim=128, max_len=200):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.max_len = max_len
        self.model = None
    
    def build_lstm_model(self):
        """Construire modÃ¨le LSTM avec embeddings"""
        self.model = Sequential([
            # Embedding layer
            Embedding(
                input_dim=self.vocab_size,
                output_dim=self.embedding_dim,
                input_length=self.max_len
            ),
            
            # Dropout
            SpatialDropout1D(0.2),
            
            # Bidirectional LSTM
            Bidirectional(LSTM(
                units=64,
                return_sequences=True,
                dropout=0.2,
                recurrent_dropout=0.2
            )),
            
            # Couche Dense intermÃ©diaire
            Dense(32, activation='relu'),
            Dropout(0.3),
            
            # Classification
            Dense(1, activation='sigmoid')
        ])
        
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', tf.keras.metrics.AUC()]
        )
        
        self.model.summary()
        return self.model
    
    def train_model(self, X_train, y_train, X_val, y_val, epochs=10):
        """EntraÃ®ner modÃ¨le"""
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=2,
                restore_best_weights=True
            ),
            ModelCheckpoint(
                'model/sentiment_model.h5',
                monitor='val_accuracy',
                save_best_only=True
            )
        ]
        
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def plot_training_history(self, history):
        """Visualiser courbes d'entraÃ®nement"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # Accuracy
        axes[0].plot(history.history['accuracy'], label='Train Accuracy')
        axes[0].plot(history.history['val_accuracy'], label='Val Accuracy')
        axes[0].set_title('Model Accuracy')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Accuracy')
        axes[0].legend()
        axes[0].grid(True)
        
        # Loss
        axes[1].plot(history.history['loss'], label='Train Loss')
        axes[1].plot(history.history['val_loss'], label='Val Loss')
        axes[1].set_title('Model Loss')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Loss')
        axes[1].legend()
        axes[1].grid(True)
        
        plt.tight_layout()
        plt.savefig('results/training_history.png', dpi=300, bbox_inches='tight')
        print("âœ… Training history plot saved!")
        return fig

# Usage
model_builder = SentimentLSTMModel(vocab_size=10000, embedding_dim=128)
model = model_builder.build_lstm_model()
history = model_builder.train_model(X_train_pad, y_train, X_test_pad, y_test)
model_builder.plot_training_history(history)
```

#### Ã‰tape 3 : Streamlit App (3-4h)

```python
# 02_DL_NLP_Sentiment/streamlit_app.py

import streamlit as st
import tensorflow as tf
import numpy as np
from src.text_preprocessing import TextPreprocessor
import plotly.express as px
import pandas as pd

# Page config
st.set_page_config(
    page_title="Sentiment Analysis App",
    page_icon="ğŸ˜Š",
    layout="wide"
)

# Styling
st.markdown("""
    <style>
        .main-title {
            font-size: 40px;
            font-weight: bold;
            color: #1f77b4;
            text-align: center;
            margin-bottom: 30px;
        }
        .subtitle {
            font-size: 18px;
            color: #555;
            text-align: center;
            margin-bottom: 20px;
        }
        .sentiment-positive {
            padding: 20px;
            background-color: #d4edda;
            border-radius: 10px;
            color: #155724;
            font-size: 18px;
            font-weight: bold;
        }
        .sentiment-negative {
            padding: 20px;
            background-color: #f8d7da;
            border-radius: 10px;
            color: #721c24;
            font-size: 18px;
            font-weight: bold;
        }
    </style>
""", unsafe_allow_html=True)

# Header
st.markdown('<div class="main-title">ğŸ˜Š IMDB Sentiment Analysis</div>', unsafe_allow_html=True)
st.markdown('<div class="subtitle">Classify movie reviews as positive or negative</div>', unsafe_allow_html=True)

# Load model
@st.cache_resource
def load_model():
    return tf.keras.models.load_model('model/sentiment_model.h5')

model = load_model()

# Preprocessor
preprocessor = TextPreprocessor(max_words=10000, max_len=200)
word_index, reverse_word_index = preprocessor.get_word_index()

# Tabs
tab1, tab2, tab3 = st.tabs(["ğŸ¬ Predict", "ğŸ“Š Analytics", "â„¹ï¸ About"])

# Tab 1: Prediction
with tab1:
    st.subheader("Enter Movie Review")
    
    user_review = st.text_area(
        "Paste your movie review here:",
        height=150,
        placeholder="E.g., This movie was absolutely amazing! The acting was superb..."
    )
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        if st.button("ğŸ” Analyze Sentiment", use_container_width=True):
            if user_review:
                # Preprocess
                cleaned = preprocessor.clean_text(user_review)
                
                # Tokenize
                sequences = []
                for word in cleaned.split():
                    if word in word_index and word_index[word] < 10000:
                        sequences.append(word_index[word])
                
                # Pad
                padded = preprocessor.pad_sequences_data(np.array([sequences]))
                
                # Predict
                prediction = model.predict(padded, verbose=0)[0][0]
                confidence = prediction if prediction > 0.5 else 1 - prediction
                sentiment = "POSITIVE âœ…" if prediction > 0.5 else "NEGATIVE âŒ"
                
                # Display
                if prediction > 0.5:
                    st.markdown(
                        f'<div class="sentiment-positive">Sentiment: {sentiment}<br>Confidence: {confidence*100:.1f}%</div>',
                        unsafe_allow_html=True
                    )
                else:
                    st.markdown(
                        f'<div class="sentiment-negative">Sentiment: {sentiment}<br>Confidence: {confidence*100:.1f}%</div>',
                        unsafe_allow_html=True
                    )
                
                # Visualize
                fig = px.bar(
                    x=['Negative', 'Positive'],
                    y=[1-prediction, prediction],
                    title="Sentiment Distribution",
                    labels={'x': 'Sentiment', 'y': 'Probability'},
                    color_discrete_map={'Negative': '#ff6b6b', 'Positive': '#51cf66'}
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("Please enter a review!")

# Tab 2: Analytics
with tab2:
    st.subheader("Model Performance Metrics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Model Accuracy",
            "92.3%",
            "+2.1%"
        )
    
    with col2:
        st.metric(
            "Precision",
            "91.5%",
            "â†‘"
        )
    
    with col3:
        st.metric(
            "Recall",
            "93.1%",
            "â†‘"
        )
    
    with col4:
        st.metric(
            "ROC-AUC",
            "0.975",
            "+0.02"
        )
    
    # Training history (dummy)
    st.subheader("Training History")
    training_data = pd.DataFrame({
        'Epoch': range(1, 11),
        'Train Accuracy': [0.60, 0.70, 0.78, 0.82, 0.85, 0.88, 0.90, 0.91, 0.92, 0.923],
        'Val Accuracy': [0.65, 0.72, 0.79, 0.83, 0.86, 0.89, 0.90, 0.91, 0.92, 0.923]
    })
    
    fig = px.line(
        training_data,
        x='Epoch',
        y=['Train Accuracy', 'Val Accuracy'],
        title="Training vs Validation Accuracy",
        labels={'value': 'Accuracy', 'variable': 'Type'}
    )
    st.plotly_chart(fig, use_container_width=True)

# Tab 3: About
with tab3:
    st.subheader("About This Model")
    
    st.markdown("""
    ### ğŸ§  Architecture
    - **Embedding Layer**: 128 dimensions word embeddings
    - **Bidirectional LSTM**: 64 units for contextual understanding
    - **Dense Layers**: 32 units + sigmoid output
    - **Dropout**: 20-30% for regularization
    
    ### ğŸ“Š Dataset
    - **IMDB Reviews**: 50,000 movie reviews
    - **Vocabulary**: 10,000 most common words
    - **Classes**: Positive (8-10 stars) / Negative (1-4 stars)
    
    ### âš™ï¸ Training
    - **Optimizer**: Adam (lr=0.001)
    - **Loss**: Binary Crossentropy
    - **Epochs**: 10 (early stopping)
    - **Batch Size**: 32
    
    ### ğŸ“ˆ Performance
    - **Train Accuracy**: 92.3%
    - **Test Accuracy**: 92.1%
    - **ROC-AUC**: 0.975
    
    ### ğŸš€ Deployment
    - **Framework**: TensorFlow/Keras
    - **Interface**: Streamlit
    - **Inference Time**: <500ms per review
    """)

st.divider()
st.markdown("Built with â¤ï¸ using TensorFlow & Streamlit")
```

#### Ã‰tape 4 : DÃ©ploiement Streamlit Cloud (2h)

```bash
# 1. CrÃ©er requirements.txt
pip freeze > requirements.txt

# 2. Push sur GitHub
git add .
git commit -m "Add sentiment analysis Streamlit app"
git push

# 3. Deploy sur Streamlit Cloud
# - Visit https://share.streamlit.io
# - Connect GitHub repo
# - Select this branch
# - Set app path: 02_DL_NLP_Sentiment/streamlit_app.py
# - Deploy!
```

**URL finale** : `https://sentiment-analysis-[username].streamlit.app`

#### Ã‰tape 5 : README Projet (30 min)

```markdown
# ğŸ§  Sentiment Analysis - Deep Learning

## Problem
Classify IMDB movie reviews as positive or negative using Deep Learning.

## Model Architecture
```
Input (200 tokens)
    â†“
Embedding (128 dims)
    â†“
Bidirectional LSTM (64 units)
    â†“
Dense (32 units, ReLU)
    â†“
Dense (1 unit, Sigmoid) â†’ Output [0-1]
```

## Performance
- **Accuracy**: 92.3%
- **Precision**: 91.5%
- **Recall**: 93.1%
- **ROC-AUC**: 0.975

## Dataset
- IMDB Reviews: 25,000 train + 25,000 test
- Binary classification (positive/negative)
- Preprocessed & tokenized

## How to Run

### Local Streamlit
```bash
streamlit run streamlit_app.py
```
Visit: http://localhost:8501

### Live App
[https://sentiment-analysis-demo.streamlit.app](https://sentiment-analysis-demo.streamlit.app)

## Files
```
02_DL_NLP_Sentiment/
â”œâ”€â”€ data/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ text_preprocessing.py
â”‚   â”œâ”€â”€ model_architecture.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ sentiment_model.h5
â”‚   â””â”€â”€ tokenizer.pickle
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 02_sentiment_analysis.ipynb
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ results/
â”‚   â””â”€â”€ training_history.png
â””â”€â”€ README.md
```

## Key Features
âœ… Real-time sentiment prediction
âœ… Confidence scores
âœ… Model performance analytics
âœ… Clean, intuitive UI
âœ… Production-ready

## Business Impact
- Can process 10k reviews/hour
- 92% accuracy on unseen data
- Deployment time: <5 mins
```

### ğŸ“Š RÃ©sultat Attendu

**Livrables** :
- âœ… Notebook Jupyter avec EDA + training
- âœ… ModÃ¨le sauvegardÃ© (.h5)
- âœ… Streamlit app live et dÃ©ployÃ©e
- âœ… Visualisations training history
- âœ… README complet
- âœ… Code modulaire propre

---

## ğŸ”§ Projet #3 : Data Engineering - ETL Pipeline

### ğŸ“Œ Objectifs

- âœ… CrÃ©er pipeline ETL automatisÃ©
- âœ… Collecter data APIs (Reddit, Twitter, Weather)
- âœ… Transformer et valider donnÃ©es
- âœ… Charger dans PostgreSQL/MongoDB
- âœ… Livrable : Architecture diagram + code + DAGs

### â±ï¸ DurÃ©e : 10-14 heures

### ğŸ”„ Ã‰tapes DÃ©taillÃ©es

#### Ã‰tape 1 : Architecture Design (1-2h)

```
Source Data (APIs)
    â†“
Extract Layer (Data Collectors)
    â†“
Transform Layer (Data Cleaners)
    â†“
Validate Layer (Quality Checks)
    â†“
Load Layer (DB Loaders)
    â†“
Data Warehouse (PostgreSQL)
```

**CrÃ©er architecture diagram** :

```python
# Using graphviz or draw.io
# 03_Data_Engineering_Pipeline/Architecture_Diagram.png

DATA_SOURCES:
  - Reddit API â†’ Extract subreddits posts
  - Twitter API â†’ Extract tweets  
  - OpenWeatherMap API â†’ Extract weather data

ETL_PIPELINE:
  - Extract: Fetch from APIs
  - Transform: Clean, deduplicate, normalize
  - Validate: Schema checks, null checks
  - Load: Insert into PostgreSQL

ORCHESTRATION:
  - Airflow DAG â†’ Schedule daily runs
  - Error handling â†’ Retry logic
  - Monitoring â†’ Logs & alerts

DATA_WAREHOUSE:
  - PostgreSQL tables
  - Indexing for performance
  - Backup strategies
```

#### Ã‰tape 2 : Data Extractors (3-4h)

```python
# 03_Data_Engineering_Pipeline/src/data_extractors.py

import praw
import tweepy
import requests
import os
from datetime import datetime
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RedditExtractor:
    """Extract Reddit data"""
    
    def __init__(self):
        self.reddit = praw.Reddit(
            client_id=os.getenv('REDDIT_CLIENT_ID'),
            client_secret=os.getenv('REDDIT_CLIENT_SECRET'),
            user_agent=os.getenv('REDDIT_USER_AGENT')
        )
    
    def extract_subreddit_posts(self, subreddit_name, limit=100):
        """Extract posts from subreddit"""
        try:
            logger.info(f"ğŸ“¥ Extracting {limit} posts from r/{subreddit_name}")
            
            subreddit = self.reddit.subreddit(subreddit_name)
            posts = []
            
            for post in subreddit.hot(limit=limit):
                posts.append({
                    'source': 'reddit',
                    'post_id': post.id,
                    'title': post.title,
                    'text': post.selftext,
                    'author': str(post.author),
                    'created_utc': datetime.fromtimestamp(post.created_utc),
                    'score': post.score,
                    'num_comments': post.num_comments,
                    'subreddit': subreddit_name,
                    'extracted_at': datetime.now()
                })
            
            logger.info(f"âœ… Extracted {len(posts)} posts")
            return posts
        
        except Exception as e:
            logger.error(f"âŒ Reddit extraction failed: {str(e)}")
            return []

class TwitterExtractor:
    """Extract Twitter data"""
    
    def __init__(self):
        self.client = tweepy.Client(
            bearer_token=os.getenv('TWITTER_BEARER_TOKEN')
        )
    
    def extract_tweets(self, query, max_results=100):
        """Extract tweets by query"""
        try:
            logger.info(f"ğŸ“¥ Extracting tweets for query: {query}")
            
            tweets_data = self.client.search_recent_tweets(
                query=query,
                max_results=max_results,
                tweet_fields=['created_at', 'public_metrics']
            )
            
            tweets = []
            if tweets_data.data:
                for tweet in tweets_data.data:
                    tweets.append({
                        'source': 'twitter',
                        'tweet_id': tweet.id,
                        'text': tweet.text,
                        'created_at': tweet.created_at,
                        'like_count': tweet.public_metrics['like_count'],
                        'retweet_count': tweet.public_metrics['retweet_count'],
                        'reply_count': tweet.public_metrics['reply_count'],
                        'query': query,
                        'extracted_at': datetime.now()
                    })
            
            logger.info(f"âœ… Extracted {len(tweets)} tweets")
            return tweets
        
        except Exception as e:
            logger.error(f"âŒ Twitter extraction failed: {str(e)}")
            return []

class WeatherExtractor:
    """Extract weather data"""
    
    def __init__(self):
        self.api_key = os.getenv('OPENWEATHER_API_KEY')
        self.base_url = 'https://api.openweathermap.org/data/2.5/weather'
    
    def extract_weather(self, city):
        """Extract weather for city"""
        try:
            logger.info(f"ğŸ“¥ Extracting weather for {city}")
            
            params = {
                'q': city,
                'appid': self.api_key,
                'units': 'metric'
            }
            
            response = requests.get(self.base_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            weather_record = {
                'source': 'weather',
                'city': city,
                'temperature': data['main']['temp'],
                'humidity': data['main']['humidity'],
                'pressure': data['main']['pressure'],
                'weather_main': data['weather'][0]['main'],
                'weather_description': data['weather'][0]['description'],
                'wind_speed': data['wind']['speed'],
                'extracted_at': datetime.now()
            }
            
            logger.info(f"âœ… Extracted weather for {city}")
            return [weather_record]
        
        except Exception as e:
            logger.error(f"âŒ Weather extraction failed: {str(e)}")
            return []

# Usage
reddit_extractor = RedditExtractor()
reddit_posts = reddit_extractor.extract_subreddit_posts('python', limit=50)

twitter_extractor = TwitterExtractor()
tweets = twitter_extractor.extract_tweets('#python', max_results=50)

weather_extractor = WeatherExtractor()
weather = weather_extractor.extract_weather('Paris')
```

#### Ã‰tape 3 : Data Transformers (2-3h)

```python
# 03_Data_Engineering_Pipeline/src/transformers.py

import pandas as pd
import re
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class DataTransformer:
    """Transform et clean raw data"""
    
    def __init__(self):
        pass
    
    def clean_text(self, text):
        """Clean text data"""
        if pd.isna(text):
            return ""
        
        # Minuscules
        text = str(text).lower()
        
        # Enlever URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Enlever mentions, hashtags
        text = re.sub(r'@\w+|#\w+', '', text)
        
        # Enlever caractÃ¨res spÃ©ciaux
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
        
        # Enlever espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def transform_reddit_posts(self, posts):
        """Transform reddit posts"""
        df = pd.DataFrame(posts)
        
        logger.info("ğŸ”„ Transforming Reddit posts...")
        
        # Clean text
        df['title'] = df['title'].apply(self.clean_text)
        df['text'] = df['text'].apply(self.clean_text)
        
        # Drop duplicates
        df = df.drop_duplicates(subset=['post_id'])
        
        # Cast types
        df['created_utc'] = pd.to_datetime(df['created_utc'])
        df['score'] = df['score'].astype(int)
        
        # Add features
        df['content_length'] = df['text'].apply(len)
        df['engagement_score'] = df['score'] + df['num_comments']
        
        logger.info(f"âœ… Transformed {len(df)} posts")
        return df
    
    def transform_tweets(self, tweets):
        """Transform tweets"""
        df = pd.DataFrame(tweets)
        
        logger.info("ğŸ”„ Transforming tweets...")
        
        # Clean text
        df['text'] = df['text'].apply(self.clean_text)
        
        # Drop duplicates
        df = df.drop_duplicates(subset=['tweet_id'])
        
        # Cast types
        df['created_at'] = pd.to_datetime(df['created_at'])
        
        # Add features
        df['engagement'] = (
            df['like_count'] + 
            df['retweet_count'] + 
            df['reply_count']
        )
        
        logger.info(f"âœ… Transformed {len(df)} tweets")
        return df
    
    def transform_weather(self, weather_records):
        """Transform weather data"""
        df = pd.DataFrame(weather_records)
        
        logger.info("ğŸ”„ Transforming weather...")
        
        # Cast types
        df['extracted_at'] = pd.to_datetime(df['extracted_at'])
        df['temperature'] = df['temperature'].astype(float)
        df['humidity'] = df['humidity'].astype(int)
        
        logger.info(f"âœ… Transformed {len(df)} weather records")
        return df

# Usage
transformer = DataTransformer()
reddit_df = transformer.transform_reddit_posts(reddit_posts)
tweets_df = transformer.transform_tweets(tweets)
weather_df = transformer.transform_weather(weather)
```

#### Ã‰tape 4 : Data Loaders (2h)

```python
# 03_Data_Engineering_Pipeline/src/loaders.py

from sqlalchemy import create_engine, Column, String, Integer, Float, DateTime, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import pandas as pd
import logging
import os

logger = logging.getLogger(__name__)

Base = declarative_base()

class RedditPost(Base):
    """ORM Model for Reddit posts"""
    __tablename__ = 'reddit_posts'
    
    post_id = Column(String, primary_key=True)
    title = Column(String)
    text = Column(Text)
    author = Column(String)
    created_utc = Column(DateTime)
    score = Column(Integer)
    num_comments = Column(Integer)
    subreddit = Column(String)
    extracted_at = Column(DateTime)

class Tweet(Base):
    """ORM Model for tweets"""
    __tablename__ = 'tweets'
    
    tweet_id = Column(String, primary_key=True)
    text = Column(Text)
    created_at = Column(DateTime)
    like_count = Column(Integer)
    retweet_count = Column(Integer)
    reply_count = Column(Integer)
    query = Column(String)
    extracted_at = Column(DateTime)

class WeatherRecord(Base):
    """ORM Model for weather"""
    __tablename__ = 'weather'
    
    id = Column(String, primary_key=True)
    city = Column(String)
    temperature = Column(Float)
    humidity = Column(Integer)
    pressure = Column(Integer)
    weather_main = Column(String)
    extracted_at = Column(DateTime)

class PostgreSQLLoader:
    """Load data into PostgreSQL"""
    
    def __init__(self):
        db_url = os.getenv(
            'DATABASE_URL',
            'postgresql://user:password@localhost:5432/datawarehouse'
        )
        self.engine = create_engine(db_url)
        Base.metadata.create_all(self.engine)
        
    def load_dataframe(self, df, table_name):
        """Load pandas dataframe to table"""
        try:
            logger.info(f"ğŸ’¾ Loading {len(df)} rows to {table_name}...")
            
            df.to_sql(
                table_name,
                con=self.engine,
                if_exists='append',
                index=False
            )
            
            logger.info(f"âœ… Loaded successfully")
        except Exception as e:
            logger.error(f"âŒ Loading failed: {str(e)}")
            raise

# Usage
loader = PostgreSQLLoader()
loader.load_dataframe(reddit_df, 'reddit_posts')
loader.load_dataframe(tweets_df, 'tweets')
loader.load_dataframe(weather_df, 'weather')
```

#### Ã‰tape 5 : Airflow DAG (2-3h)

```python
# 03_Data_Engineering_Pipeline/dags/data_pipeline_dag.py

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
import sys

sys.path.insert(0, '/opt/airflow/dags/../..')

from src.data_extractors import RedditExtractor, TwitterExtractor, WeatherExtractor
from src.transformers import DataTransformer
from src.loaders import PostgreSQLLoader

# Default args
default_args = {
    'owner': 'data-engineer',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': days_ago(1),
}

# DAG definition
dag = DAG(
    'data_pipeline_dag',
    default_args=default_args,
    description='ETL pipeline: Reddit, Twitter, Weather',
    schedule_interval='@daily',  # Run daily
    catchup=False,
)

# Extract tasks
def extract_reddit_task():
    extractor = RedditExtractor()
    posts = extractor.extract_subreddit_posts('python', limit=100)
    return posts

def extract_twitter_task():
    extractor = TwitterExtractor()
    tweets = extractor.extract_tweets('#dataengineering', max_results=100)
    return tweets

def extract_weather_task():
    extractor = WeatherExtractor()
    weather = extractor.extract_weather('Paris')
    return weather

# Transform task
def transform_task(ti):
    reddit_posts = ti.xcom_pull(task_ids='extract_reddit')
    tweets = ti.xcom_pull(task_ids='extract_twitter')
    weather = ti.xcom_pull(task_ids='extract_weather')
    
    transformer = DataTransformer()
    
    reddit_df = transformer.transform_reddit_posts(reddit_posts)
    tweets_df = transformer.transform_tweets(tweets)
    weather_df = transformer.transform_weather(weather)
    
    # Store in XCom
    ti.xcom_push(key='reddit_df', value=reddit_df.to_json())
    ti.xcom_push(key='tweets_df', value=tweets_df.to_json())
    ti.xcom_push(key='weather_df', value=weather_df.to_json())

# Load task
def load_task(ti):
    import pandas as pd
    
    reddit_json = ti.xcom_pull(key='reddit_df', task_ids='transform')
    tweets_json = ti.xcom_pull(key='tweets_df', task_ids='transform')
    weather_json = ti.xcom_pull(key='weather_df', task_ids='transform')
    
    reddit_df = pd.read_json(reddit_json)
    tweets_df = pd.read_json(tweets_json)
    weather_df = pd.read_json(weather_json)
    
    loader = PostgreSQLLoader()
    loader.load_dataframe(reddit_df, 'reddit_posts')
    loader.load_dataframe(tweets_df, 'tweets')
    loader.load_dataframe(weather_df, 'weather')

# Create tasks
extract_reddit = PythonOperator(
    task_id='extract_reddit',
    python_callable=extract_reddit_task,
    dag=dag,
)

extract_twitter = PythonOperator(
    task_id='extract_twitter',
    python_callable=extract_twitter_task,
    dag=dag,
)

extract_weather = PythonOperator(
    task_id='extract_weather',
    python_callable=extract_weather_task,
    dag=dag,
)

transform = PythonOperator(
    task_id='transform',
    python_callable=transform_task,
    dag=dag,
)

load = PythonOperator(
    task_id='load',
    python_callable=load_task,
    dag=dag,
)

# Set dependencies
[extract_reddit, extract_twitter, extract_weather] >> transform >> load
```

#### Ã‰tape 6 : SQL Schema (1h)

```sql
-- 03_Data_Engineering_Pipeline/sql_scripts/schema_creation.sql

-- Create tables
CREATE TABLE reddit_posts (
    post_id VARCHAR(255) PRIMARY KEY,
    title VARCHAR(500),
    text TEXT,
    author VARCHAR(100),
    created_utc TIMESTAMP,
    score INTEGER,
    num_comments INTEGER,
    subreddit VARCHAR(100),
    content_length INTEGER,
    engagement_score INTEGER,
    extracted_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tweets (
    tweet_id VARCHAR(255) PRIMARY KEY,
    text TEXT,
    created_at TIMESTAMP,
    like_count INTEGER,
    retweet_count INTEGER,
    reply_count INTEGER,
    engagement INTEGER,
    query VARCHAR(255),
    extracted_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE weather (
    id VARCHAR(255) PRIMARY KEY,
    city VARCHAR(100),
    temperature FLOAT,
    humidity INTEGER,
    pressure INTEGER,
    weather_main VARCHAR(50),
    weather_description VARCHAR(255),
    wind_speed FLOAT,
    extracted_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create indexes
CREATE INDEX idx_reddit_created ON reddit_posts(created_utc);
CREATE INDEX idx_reddit_subreddit ON reddit_posts(subreddit);
CREATE INDEX idx_tweets_created ON tweets(created_at);
CREATE INDEX idx_weather_city ON weather(city);

-- Data quality checks
SELECT COUNT(*) as reddit_count FROM reddit_posts;
SELECT COUNT(*) as tweets_count FROM tweets;
SELECT COUNT(*) as weather_count FROM weather;
```

#### Ã‰tape 7 : README Projet (30 min)

```markdown
# ğŸ”§ Data Engineering - ETL Pipeline

## Overview
Automated ETL pipeline collecting data from multiple sources (Reddit, Twitter, Weather APIs), transforming, validating, and loading into PostgreSQL.

## Architecture

```
EXTRACT â†’ TRANSFORM â†’ VALIDATE â†’ LOAD
   â†“          â†“           â†“        â†“
 Reddit     Clean      Schema    PostgreSQL
 Twitter    Dedupe     Checks    Database
 Weather    Normalize  Logging   Warehouse
```

## Components

### 1. Data Extractors
- **RedditExtractor**: Fetch posts from subreddits
- **TwitterExtractor**: Search tweets by query
- **WeatherExtractor**: Get weather data from APIs

### 2. Transformers
- Text cleaning & preprocessing
- Deduplication
- Type casting
- Feature engineering

### 3. Loaders
- PostgreSQL connection management
- ORM models (SQLAlchemy)
- Batch loading

### 4. Orchestration
- Apache Airflow DAGs
- Daily scheduling
- Error handling & retries
- Monitoring & alerts

## Data Flow

```
APIs (Reddit, Twitter, Weather)
       â†“â†“â†“
Airflow Extract Tasks
       â†“â†“â†“
Airflow Transform Task
       â†“â†“â†“
Airflow Load Task
       â†“â†“â†“
PostgreSQL Tables
(reddit_posts, tweets, weather)
```

## Setup

### Prerequisites
- Python 3.10+
- PostgreSQL 12+
- Apache Airflow 2.6+
- API credentials (Reddit, Twitter, OpenWeatherMap)

### Installation

1. **Create .env file**
```bash
REDDIT_CLIENT_ID=xxx
REDDIT_CLIENT_SECRET=xxx
TWITTER_BEARER_TOKEN=xxx
OPENWEATHER_API_KEY=xxx
DATABASE_URL=postgresql://user:pass@localhost:5432/datawarehouse
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Initialize Airflow**
```bash
airflow db init
airflow users create --username admin --password admin
```

4. **Start Airflow**
```bash
airflow webserver  # Port 8080
airflow scheduler  # Separate terminal
```

5. **Deploy DAG**
```bash
cp dags/data_pipeline_dag.py ~/airflow/dags/
```

## Database Schema

### reddit_posts
- post_id (PK)
- title, text
- author, subreddit
- created_utc, score, num_comments
- content_length, engagement_score

### tweets
- tweet_id (PK)
- text, query
- created_at
- like_count, retweet_count, reply_count
- engagement

### weather
- id (PK)
- city, temperature, humidity
- pressure, weather_main
- extracted_at

## Monitoring

### Airflow Dashboard
- Visit: http://localhost:8080
- Monitor task status, logs, performance

### Data Quality
- Check row counts
- Validate NULL values
- Monitor for duplicates

### Performance
- Track extraction time
- Monitor database query performance
- Alert on pipeline failures

## Deployment

### Production Setup
```bash
# Use managed Airflow (AWS MWAA, GCP Cloud Composer)
# Docker containers for scalability
# RDS for PostgreSQL
# CloudWatch for monitoring
```

## Files Structure
```
03_Data_Engineering_Pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_extractors.py
â”‚   â”œâ”€â”€ transformers.py
â”‚   â””â”€â”€ loaders.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_pipeline_dag.py
â”œâ”€â”€ sql_scripts/
â”‚   â”œâ”€â”€ schema_creation.sql
â”‚   â””â”€â”€ data_quality_checks.sql
â”œâ”€â”€ config/
â”‚   â””â”€â”€ airflow_config.yaml
â”œâ”€â”€ Architecture_Diagram.png
â””â”€â”€ README.md
```

## Key Features
âœ… Automated daily runs
âœ… Multi-source data collection
âœ… Data validation & cleaning
âœ… Error handling & retries
âœ… Production-ready
âœ… Scalable architecture

## Performance
- **Extraction**: 2-5 min (100 records/source)
- **Transformation**: 1-2 min
- **Loading**: <1 min
- **Total DAG runtime**: ~10 minutes

## Next Steps
- Add more data sources
- Implement data lineage tracking
- Add alerting for data quality issues
- Set up data warehouse analytics
```

### ğŸ“Š RÃ©sultat Attendu

**Livrables** :
- âœ… Architecture diagram visuel
- âœ… Code extractors, transformers, loaders
- âœ… Airflow DAG automatisÃ©
- âœ… SQL schema + donnÃ©es
- âœ… Configuration .env
- âœ… Documentation complÃ¨te

---

## ğŸ“Š Projet #4 : Business-Focused - Dashboard Recommandation

### ğŸ“Œ Objectifs

- âœ… CrÃ©er systÃ¨me recommandation produits
- âœ… Dashboard interactif Plotly/Dash
- âœ… DÃ©ployer sur Heroku live
- âœ… Livrable : App + business case

### â±ï¸ DurÃ©e : 8-12 heures

#### Ã‰tape 1-4 : Implementation complÃ¨te

```python
# 04_Dashboard_Recommendation/dashboard/app.py

import dash
from dash import dcc, html, Input, Output, State
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import numpy as np
from src.recommendation_engine import RecommendationEngine
import logging

# Setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize app
app = dash.Dash(__name__)
app.title = "Product Recommendation Engine"

# Initialize recommendation engine
rec_engine = RecommendationEngine()
products_df = rec_engine.load_products()

# Styling
external_stylesheets = [
    'https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap'
]

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)

# Global colors
colors = {
    'primary': '#1f77b4',
    'secondary': '#ff7f0e',
    'success': '#2ca02c',
    'danger': '#d62728',
    'background': '#f8f9fa',
    'text': '#2c3e50'
}

# App layout
app.layout = html.Div([
    # Header
    html.Div([
        html.Div([
            html.H1("ğŸ›ï¸ Smart Product Recommender", style={
                'color': colors['primary'],
                'margin': 0,
                'fontSize': '32px',
                'fontWeight': '700'
            }),
            html.P("AI-powered personalized product recommendations", style={
                'color': colors['text'],
                'margin': '5px 0 0 0',
                'fontSize': '14px'
            })
        ], style={'flex': 1}),
        
        html.Div([
            html.Div([
                html.Span("Total Products:", style={'fontWeight': '600'}),
                html.Span(f" {len(products_df)}", style={'fontSize': '18px', 'color': colors['primary']})
            ], style={'marginBottom': '10px'}),
            
            html.Div([
                html.Span("Avg Rating:", style={'fontWeight': '600'}),
                html.Span(f" {products_df['rating'].mean():.2f}â­", style={'fontSize': '18px', 'color': colors['success']})
            ])
        ], style={'textAlign': 'right'})
    ], style={
        'display': 'flex',
        'justifyContent': 'space-between',
        'alignItems': 'center',
        'padding': '20px',
        'backgroundColor': 'white',
        'borderBottom': f'2px solid {colors["primary"]}',
        'marginBottom': '20px'
    }),
    
    # Main content
    html.Div([
        # Left sidebar - Input
        html.Div([
            html.Div([
                html.H3("ğŸ¯ Select Product", style={'color': colors['primary']}),
                
                dcc.Dropdown(
                    id='product-dropdown',
                    options=[
                        {'label': f"{row['name']} (â­ {row['rating']})", 'value': row['product_id']}
                        for _, row in products_df.iterrows()
                    ],
                    placeholder="Choose a product...",
                    style={'width': '100%'}
                ),
                
                html.Button(
                    "Get Recommendations",
                    id='recommend-btn',
                    n_clicks=0,
                    style={
                        'width': '100%',
                        'padding': '10px',
                        'marginTop': '15px',
                        'backgroundColor': colors['primary'],
                        'color': 'white',
                        'border': 'none',
                        'borderRadius': '5px',
                        'fontSize': '14px',
                        'fontWeight': '600',
                        'cursor': 'pointer'
                    }
                ),
                
                html.Div(id='selected-product', style={
                    'marginTop': '20px',
                    'padding': '15px',
                    'backgroundColor': colors['background'],
                    'borderRadius': '5px'
                })
                
            ], style={
                'backgroundColor': 'white',
                'padding': '20px',
                'borderRadius': '8px',
                'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'
            })
            
        ], style={
            'width': '25%',
            'marginRight': '20px'
        }),
        
        # Right content - Results
        html.Div([
            # Recommendations cards
            html.Div(id='recommendations-container', style={
                'display': 'grid',
                'gridTemplateColumns': 'repeat(auto-fill, minmax(200px, 1fr))',
                'gap': '15px',
                'marginBottom': '30px'
            }),
            
            # Analytics section
            html.Div([
                html.H3("ğŸ“Š Recommendation Analytics", style={'color': colors['primary']}),
                
                html.Div([
                    # Chart 1: Similarity scores
                    html.Div([
                        dcc.Graph(id='similarity-chart')
                    ], style={'flex': 1}),
                    
                    # Chart 2: Category distribution
                    html.Div([
                        dcc.Graph(id='category-chart')
                    ], style={'flex': 1})
                ], style={'display': 'flex', 'gap': '20px', 'marginBottom': '20px'}),
                
                # Chart 3: Price analysis
                html.Div([
                    dcc.Graph(id='price-chart', style={'width': '100%'})
                ])
                
            ], style={
                'backgroundColor': 'white',
                'padding': '20px',
                'borderRadius': '8px',
                'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'
            })
            
        ], style={'flex': 1})
        
    ], style={
        'display': 'flex',
        'padding': '20px'
    }),
    
    # Store for data
    dcc.Store(id='recommendations-store')
    
], style={
    'fontFamily': "'Inter', sans-serif",
    'backgroundColor': colors['background'],
    'minHeight': '100vh',
    'padding': '0'
})

# Callbacks
@app.callback(
    [Output('selected-product', 'children'),
     Output('recommendations-store', 'data')],
    [Input('recommend-btn', 'n_clicks')],
    [State('product-dropdown', 'value')],
    prevent_initial_call=True
)
def update_recommendations(n_clicks, selected_product):
    if not selected_product:
        return "Please select a product", None
    
    # Get selected product info
    selected = products_df[products_df['product_id'] == selected_product].iloc[0]
    
    # Get recommendations
    recommendations = rec_engine.recommend(selected_product, n_recommendations=5)
    
    # Store for charts
    recommendations_data = {
        'selected': selected.to_dict(),
        'recommendations': [
            {
                'product_id': r[0],
                'score': float(r[1]),
                'name': products_df[products_df['product_id'] == r[0]]['name'].values[0]
            }
            for r in recommendations
        ]
    }
    
    # Display selected product
    selected_html = html.Div([
        html.H4(selected['name']),
        html.P(f"Category: {selected['category']}"),
        html.P(f"Price: ${selected['price']:.2f}"),
        html.P(f"Rating: {selected['rating']:.2f}â­"),
        html.P(f"Reviews: {selected['num_reviews']}")
    ])
    
    return selected_html, recommendations_data

@app.callback(
    Output('recommendations-container', 'children'),
    Input('recommendations-store', 'data')
)
def update_recommendation_cards(data):
    if not data:
        return []
    
    cards = []
    for rec in data['recommendations']:
        product = products_df[products_df['product_id'] == rec['product_id']].iloc[0]
        
        card = html.Div([
            html.H4(rec['name'], style={'fontSize': '14px', 'marginTop': 0}),
            html.P(f"Similarity: {rec['score']:.2%}", style={
                'color': colors['success'],
                'fontWeight': '600',
                'fontSize': '12px'
            }),
            html.P(f"${product['price']:.2f}", style={'fontWeight': '600', 'fontSize': '14px'}),
            html.P(f"{product['rating']:.2f}â­", style={'fontSize': '12px', 'color': colors['primary']}),
            html.Button("View Details", style={
                'width': '100%',
                'padding': '8px',
                'backgroundColor': colors['primary'],
                'color': 'white',
                'border': 'none',
                'borderRadius': '4px',
                'fontSize': '12px',
                'cursor': 'pointer'
            })
        ], style={
            'backgroundColor': 'white',
            'padding': '15px',
            'borderRadius': '8px',
            'textAlign': 'center',
            'boxShadow': '0 2px 4px rgba(0,0,0,0.1)',
            'border': f'2px solid {colors["primary"]}'
        })
        
        cards.append(card)
    
    return cards

@app.callback(
    Output('similarity-chart', 'figure'),
    Input('recommendations-store', 'data')
)
def update_similarity_chart(data):
    if not data:
        return {}
    
    names = [rec['name'] for rec in data['recommendations']]
    scores = [rec['score'] for rec in data['recommendations']]
    
    fig = go.Figure(data=[
        go.Bar(
            x=names,
            y=scores,
            marker_color=colors['primary'],
            text=[f'{s:.2%}' for s in scores],
            textposition='outside'
        )
    ])
    
    fig.update_layout(
        title="Similarity Scores",
        xaxis_title="Product",
        yaxis_title="Similarity",
        showlegend=False,
        height=300
    )
    
    return fig

# Run app
if __name__ == '__main__':
    app.run_server(debug=True, port=8050)
```

**Procufile pour Heroku** :

```
web: gunicorn dashboard.app:server
```

**Deployment** :

```bash
heroku login
heroku create my-recommender-app
git push heroku main
# Visit: https://my-recommender-app.herokuapp.com
```

#### README Projet :

```markdown
# ğŸ’¼ Product Recommendation Dashboard

## Problem
E-commerce businesses need intelligent ways to recommend products to increase sales.

## Solution
ML-powered dashboard recommending similar products based on content similarity and user preferences.

## Features
- ğŸ¯ Intelligent product recommendations
- ğŸ“Š Interactive analytics dashboard
- âš¡ Real-time computations
- ğŸ“± Responsive design
- ğŸš€ Production deployment

## Algorithm
- Content-based filtering
- Cosine similarity on product features
- Feature engineering (price, category, ratings)

## Tech Stack
- Python + Plotly/Dash
- Scikit-learn for ML
- Heroku for deployment

## Live Demo
[https://product-recommender.herokuapp.com](https://product-recommender.herokuapp.com)

## Business Impact
- +25% average order value
- +40% click-through rate
- Reduced product discovery time
```

---

## ğŸŒ Site Web Portfolio Interactif

### Structure HTML/CSS/JS

```html
<!-- portfolio_website/index.html -->

<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Science Portfolio - [Ton nom]</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="container">
            <div class="logo">Portfolio</div>
            <ul class="nav-links">
                <li><a href="#projects">Projets</a></li>
                <li><a href="#skills">CompÃ©tences</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </div>
    </nav>
    
    <!-- Hero Section -->
    <section class="hero">
        <div class="hero-content">
            <h1>Data Scientist en Formation</h1>
            <p>ML | Deep Learning | Data Engineering</p>
            <a href="#projects" class="btn">Voir mes projets</a>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects" class="projects">
        <h2>ğŸ“Š Mes 4 Projets Portfolio</h2>
        
        <div class="project-grid">
            <!-- Project 1 -->
            <div class="project-card">
                <div class="project-header">
                    <h3>ğŸ¤– Churn Prediction</h3>
                    <span class="tag">Machine Learning</span>
                </div>
                <p>PrÃ©diction clients churn avec Random Forest + XGBoost. Accuracy 87.2%.</p>
                <div class="project-links">
                    <a href="https://github.com/[username]/portfolio/tree/main/01_ML_Classique_Churn" target="_blank">GitHub</a>
                    <a href="https://[username]-churn.herokuapp.com" target="_blank">Live</a>
                </div>
            </div>
            
            <!-- Project 2 -->
            <div class="project-card">
                <div class="project-header">
                    <h3>ğŸ§  Sentiment Analysis</h3>
                    <span class="tag">Deep Learning</span>
                </div>
                <p>LSTM Bidirectionnel NLP sentiment IMDB. Accuracy 92.3%. Streamlit app.</p>
                <div class="project-links">
                    <a href="https://github.com/[username]/portfolio/tree/main/02_DL_NLP_Sentiment" target="_blank">GitHub</a>
                    <a href="https://sentiment-analysis-[username].streamlit.app" target="_blank">Live</a>
                </div>
            </div>
            
            <!-- Project 3 -->
            <div class="project-card">
                <div class="project-header">
                    <h3>ğŸ”§ ETL Pipeline</h3>
                    <span class="tag">Data Engineering</span>
                </div>
                <p>Pipeline automatisÃ© Airflow. Extraction Reddit/Twitter/Weather. PostgreSQL.</p>
                <div class="project-links">
                    <a href="https://github.com/[username]/portfolio/tree/main/03_Data_Engineering_Pipeline" target="_blank">GitHub</a>
                    <a href="#">Documentation</a>
                </div>
            </div>
            
            <!-- Project 4 -->
            <div class="project-card">
                <div class="project-header">
                    <h3>ğŸ’¼ Recommender Dashboard</h3>
                    <span class="tag">Business Focus</span>
                </div>
                <p>Dashboard Dash produit recommandation. Similarity scoring. Heroku live.</p>
                <div class="project-links">
                    <a href="https://github.com/[username]/portfolio/tree/main/04_Dashboard_Recommendation" target="_blank">GitHub</a>
                    <a href="https://product-recommender-[username].herokuapp.com" target="_blank">Live</a>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Skills Section -->
    <section id="skills" class="skills">
        <h2>ğŸ› ï¸ CompÃ©tences Techniques</h2>
        
        <div class="skills-grid">
            <div class="skill-category">
                <h3>Languages</h3>
                <div class="skill-tags">
                    <span>Python</span>
                    <span>SQL</span>
                    <span>JavaScript</span>
                </div>
            </div>
            
            <div class="skill-category">
                <h3>ML/DL</h3>
                <div class="skill-tags">
                    <span>scikit-learn</span>
                    <span>TensorFlow</span>
                    <span>PyTorch</span>
                </div>
            </div>
            
            <div class="skill-category">
                <h3>Data</h3>
                <div class="skill-tags">
                    <span>pandas</span>
                    <span>PostgreSQL</span>
                    <span>Airflow</span>
                </div>
            </div>
            
            <div class="skill-category">
                <h3>Deployment</h3>
                <div class="skill-tags">
                    <span>Git</span>
                    <span>Docker</span>
                    <span>Heroku</span>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Contact Section -->
    <section id="contact" class="contact">
        <h2>ğŸ“§ Get in Touch</h2>
        
        <div class="contact-content">
            <p>IntÃ©ressÃ© par collaboration ou discussion sur data science?</p>
            
            <div class="contact-links">
                <a href="https://github.com/[username]" target="_blank">GitHub</a>
                <a href="https://linkedin.com/in/[username]" target="_blank">LinkedIn</a>
                <a href="mailto:[email]">Email</a>
            </div>
        </div>
    </section>
    
    <!-- Footer -->
    <footer>
        <p>&copy; 2026 Data Science Portfolio. Built with â¤ï¸</p>
    </footer>
    
    <script src="js/main.js"></script>
</body>
</html>
```

```css
/* portfolio_website/css/style.css */

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: 'Inter', sans-serif;
    line-height: 1.6;
    color: #2c3e50;
    background-color: #f8f9fa;
}

/* Navbar */
.navbar {
    background: white;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    position: sticky;
    top: 0;
    z-index: 100;
}

.navbar .container {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1rem 2rem;
    max-width: 1200px;
    margin: 0 auto;
}

.logo {
    font-size: 1.5rem;
    font-weight: 700;
    color: #1f77b4;
}

.nav-links {
    display: flex;
    list-style: none;
    gap: 2rem;
}

.nav-links a {
    text-decoration: none;
    color: #2c3e50;
    transition: color 0.3s;
}

.nav-links a:hover {
    color: #1f77b4;
}

/* Hero */
.hero {
    background: linear-gradient(135deg, #1f77b4 0%, #ff7f0e 100%);
    color: white;
    padding: 8rem 2rem;
    text-align: center;
}

.hero-content h1 {
    font-size: 3rem;
    margin-bottom: 1rem;
}

.hero-content p {
    font-size: 1.3rem;
    margin-bottom: 2rem;
}

.btn {
    display: inline-block;
    background: white;
    color: #1f77b4;
    padding: 0.8rem 2rem;
    border-radius: 5px;
    text-decoration: none;
    font-weight: 600;
    transition: transform 0.3s, box-shadow 0.3s;
}

.btn:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.2);
}

/* Projects */
.projects {
    padding: 4rem 2rem;
    max-width: 1200px;
    margin: 0 auto;
}

.projects h2 {
    font-size: 2rem;
    margin-bottom: 2rem;
    text-align: center;
    color: #1f77b4;
}

.project-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 2rem;
}

.project-card {
    background: white;
    border-radius: 8px;
    padding: 1.5rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    transition: box-shadow 0.3s, transform 0.3s;
    border: 2px solid #1f77b4;
}

.project-card:hover {
    box-shadow: 0 8px 16px rgba(0,0,0,0.2);
    transform: translateY(-4px);
}

.project-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 1rem;
}

.project-header h3 {
    color: #1f77b4;
}

.tag {
    background: #ff7f0e;
    color: white;
    padding: 0.25rem 0.75rem;
    border-radius: 20px;
    font-size: 0.8rem;
    font-weight: 600;
}

.project-links {
    display: flex;
    gap: 1rem;
    margin-top: 1rem;
}

.project-links a {
    flex: 1;
    display: inline-block;
    background: #1f77b4;
    color: white;
    padding: 0.5rem;
    border-radius: 4px;
    text-decoration: none;
    text-align: center;
    font-size: 0.9rem;
    transition: background 0.3s;
}

.project-links a:hover {
    background: #ff7f0e;
}

/* Skills */
.skills {
    background: white;
    padding: 4rem 2rem;
    margin: 2rem 0;
}

.skills h2 {
    font-size: 2rem;
    margin-bottom: 2rem;
    text-align: center;
    color: #1f77b4;
}

.skills-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
    gap: 2rem;
    max-width: 1200px;
    margin: 0 auto;
}

.skill-category h3 {
    color: #1f77b4;
    margin-bottom: 1rem;
}

.skill-tags {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
}

.skill-tags span {
    background: #f0f0f0;
    padding: 0.5rem 1rem;
    border-radius: 20px;
    font-size: 0.9rem;
    border: 1px solid #1f77b4;
}

/* Contact */
.contact {
    padding: 4rem 2rem;
    text-align: center;
    max-width: 800px;
    margin: 0 auto;
}

.contact h2 {
    font-size: 2rem;
    margin-bottom: 1rem;
    color: #1f77b4;
}

.contact-content p {
    font-size: 1.1rem;
    margin-bottom: 2rem;
    color: #555;
}

.contact-links {
    display: flex;
    gap: 1rem;
    justify-content: center;
}

.contact-links a {
    background: #1f77b4;
    color: white;
    padding: 0.8rem 1.5rem;
    border-radius: 5px;
    text-decoration: none;
    font-weight: 600;
    transition: background 0.3s;
}

.contact-links a:hover {
    background: #ff7f0e;
}

/* Footer */
footer {
    background: #2c3e50;
    color: white;
    text-align: center;
    padding: 2rem;
}

/* Responsive */
@media (max-width: 768px) {
    .hero-content h1 {
        font-size: 2rem;
    }
    
    .nav-links {
        gap: 1rem;
    }
    
    .project-grid {
        grid-template-columns: 1fr;
    }
}
```

---

## âœ… Checklist Finalisation

### Avant de Pousser sur GitHub

- [ ] Tous les notebooks Jupyter testÃ©s et exÃ©cutables
- [ ] Tous les scripts Python testÃ©s localement
- [ ] README complets pour chaque projet
- [ ] .gitignore crÃ©Ã© (donnÃ©es sensibles, credentials)
- [ ] requirements.txt actualisÃ©

### GitHub Repository

- [ ] Repository crÃ©Ã© (data-science-portfolio)
- [ ] README.md racine complet
- [ ] Tous les projets dans dossiers sÃ©parÃ©s
- [ ] Portfolio website dÃ©ployÃ©e (GitHub Pages)
- [ ] Tous les liens GitHub/live fonctionnent

### DÃ©ploiements

- [ ] Projet #2 Streamlit live et stable
- [ ] Projet #4 Heroku live et stable
- [ ] Domaine personnalisÃ© (optionnel)

### LinkedIn & RÃ©seau

- [ ] LinkedIn profile actualisÃ© avec projets
- [ ] Portfolio website link dans bio
- [ ] Posts sur chaque projet
- [ ] Engagement communautÃ© data

### PrÃ©paration Recrutement

- [ ] CV mis Ã  jour avec tous les projets
- [ ] PDF brefs de chaque projet
- [ ] Quick elevator pitch prÃªt
- [ ] Questions techniques prÃ©parÃ©es

---

## ğŸ¯ RÃ©sumÃ© ExÃ©cution

**DurÃ©e totale** : 40-54h sur 4-6 semaines

**SÃ©quence optimale** :

**Semaine 1-2** : Projet #1 (ML)  
**Semaine 2-3** : Projet #2 (DL + Streamlit)  
**Semaine 4** : Projet #3 (Data Eng) + Projet #4 dÃ©but  
**Semaine 4-5** : Projet #4 (Dashboard + Heroku)  
**Semaine 5-6** : Portfolio website + Finalisation  

**Impact** : Portfolio visible, impressionnant, CDI-winning ğŸš€

---

**C'est l'heure d'exÃ©cuter. Bonne chance!**
